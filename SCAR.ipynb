{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SCAR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMntJW2rk+QNkyv7fwNMi5A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yazeedMohi/SCAR-Scene-Captured-Arabic-Recognition/blob/master/SCAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW8fvJCs3hmp",
        "colab_type": "text"
      },
      "source": [
        "# **SCAR (Scene Calibrated Arabic Recognizer)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzhHyFOw34AJ",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9jfAjbW3fcW",
        "colab_type": "code",
        "outputId": "bfdafda4-4b8b-4c2b-a1ea-bc27c23f6ec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%tensorflow_version 2.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gtxjJmVJm-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SUX2XYu4BC5",
        "colab_type": "text"
      },
      "source": [
        "# Run Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbp3R1rg4DIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main1(image=\"/content/drive/My Drive/data/demo/photo5.jpg\",localizer = \"east\",  recognizer = \"yazeed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLzeCAVV4ELe",
        "colab_type": "text"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ipsoRa04GSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "class CFG:\n",
        "    def __init__(self):\n",
        "        self.initial_epoch = 0\n",
        "        self.validation_split_ratio = 0.1\n",
        "        self.max_train_img_size = int(736)\n",
        "        self.max_predict_img_size = int(1536)\n",
        "        assert self.max_train_img_size in [256, 384, 512, 640, 736], \\\n",
        "            'max_train_img_size must in [256, 384, 512, 640, 736]'\n",
        "        if self.max_train_img_size == 256:\n",
        "            self.batch_size = 8\n",
        "        elif self.max_train_img_size == 384:\n",
        "            self.batch_size = 4\n",
        "        elif self.max_train_img_size == 512:\n",
        "            self.batch_size = 2\n",
        "        else:\n",
        "            self.batch_size = 1\n",
        "        \n",
        "        self.origin_image_dir_name = 'Images/Train/'\n",
        "        self.origin_txt_dir_name = 'GroundTruths/Train/'\n",
        "        self.origin_image_dir_name_test = 'Images/Test/'\n",
        "        self.origin_txt_dir_name_test = 'GroundTruths/Test/'\n",
        "        \n",
        "        # in paper it's 0.3, maybe to large to this problem\n",
        "        self.shrink_ratio = 0.2\n",
        "        # pixels between 0.2 and 0.6 are side pixels\n",
        "        self.shrink_side_ratio = 0.6\n",
        "        self.epsilon = 1e-4\n",
        "\n",
        "        self.num_channels = 3\n",
        "        self.feature_layers_range = range(5, 1, -1)\n",
        "        self.feature_layers_num = len(self.feature_layers_range)\n",
        "        self.pixel_size = 2 ** self.feature_layers_range[-1]\n",
        "        self.locked_layers = False\n",
        "\n",
        "        \n",
        "        \n",
        "        self.pixel_threshold = 0.9\n",
        "        self.side_vertex_pixel_threshold = 0.9\n",
        "        self.trunc_threshold = 0.1\n",
        "        self.predict_cut_text_line = True\n",
        "        self.predict_write_txt = False\n",
        "\n",
        "        self.greedy=False\n",
        "        self.beam_width=10\n",
        "        self.top_paths=1\n",
        "cfg = CFG()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6KjxhgK4T_7",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnw7GWzC4WlJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import h5py\n",
        "import os\n",
        "import string\n",
        "import datetime\n",
        "\n",
        "def main1(image,localizer = None,loc_source = \"icpr\", recognizer = None,rec_source=\"khattx\", replacement = False):\n",
        "\n",
        "    if(localizer):\n",
        "        target_path = os.path.join(\"/content/drive/My Drive/\", \"output\", loc_source, localizer, \"checkpoint_weights.hdf5\")\n",
        "        loc_model = NNModel_loc(architecture=localizer)\n",
        "        loc_model.compile()\n",
        "        loc_model.load_checkpoint(target=target_path)\n",
        "\n",
        "        print('3')\n",
        "    if(recognizer):\n",
        "        if(rec_source.find(\"khatt\") != -1):\n",
        "            arabic = True\n",
        "            # Best size for KHATT & KHATTX\n",
        "            input_size = (1024, 64, 1)\n",
        "            charset_base = 'ءآأإابتةثجحخدذرزسشصضطظعغفقكلمنؤهويىئ0123456789@:,.?!\"()//\\=-_#%$^&*+ '\n",
        "        else:\n",
        "            arabic = False\n",
        "            input_size = (1024, 128, 1)\n",
        "            charset_base = string.printable[:95]\n",
        "        max_text_length = 128\n",
        "        tokenizer = Tokenizer(chars=charset_base, max_text_length=max_text_length)\n",
        "        target_path = os.path.join(\"/content/drive/My Drive/\", \"output\", rec_source, recognizer, \"checkpoint_weights.hdf5\")\n",
        "        rec_model = NNModel_rec(architecture=recognizer,\n",
        "                                input_size=input_size,\n",
        "                                vocab_size=tokenizer.vocab_size,\n",
        "                                top_paths=10)\n",
        "        rec_model.compile()\n",
        "        rec_model.load_checkpoint(target=target_path) \n",
        "    \n",
        "    input_images = []\n",
        "    segmented = None\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    if(localizer):\n",
        "        print(\"Localizing...\")\n",
        "        input_images, segmented = predict_image(loc_model.model, image, cfg.pixel_threshold)\n",
        "        cv2_imshow(segmented)\n",
        "    else:\n",
        "        img = preprocess(cv2.imread(image), input_size=input_size)\n",
        "        print(img.shape)\n",
        "        input_images = [img]\n",
        "    if(recognizer):\n",
        "        print(\"Recognizing...\")\n",
        "        print(input_images[0].shape)\n",
        "        print(np.asarray(input_images).shape)\n",
        "        predicts, _ = predict_text(model = rec_model.model,x=input_images, ctc_decode=True)\n",
        "        predicts = [tokenizer.decode(x[0]) for x in predicts]\n",
        "        for pd, i in zip(predicts,range(len(predicts))):\n",
        "          cv2_imshow(adjust_to_see(input_images[i]))\n",
        "          print(str(pd))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grlsMJwN4HNf",
        "colab_type": "text"
      },
      "source": [
        "# Localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "699y_sRri2qB",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHeuj6u_i2P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "NNModel Class\n",
        "The NNModel class use Tensorflow Keras module.\n",
        "\n",
        "x is the input features and y the labels.\n",
        "\"\"\"\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Concatenate, Conv2D, UpSampling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "##SET DEFS\n",
        "class NNModel_loc:\n",
        "\n",
        "    def __init__(self,\n",
        "                 architecture=\"east\",\n",
        "                 num_channels=3,\n",
        "                 locked_layers=False,\n",
        "                 feature_layers_range=range(5, 1, -1),\n",
        "                 feature_layers_num=4,\n",
        "                 learning_rate=1e-3\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Initialization of a NN Model.\n",
        "\n",
        "        parameters:\n",
        "            architecture: option of the architecture model to build and compile.\n",
        "        \"\"\"\n",
        "\n",
        "        self.architecture = globals()[architecture]\n",
        "        self.num_channels = num_channels\n",
        "        self.locked_layers = locked_layers\n",
        "        self.feature_layers_range = feature_layers_range\n",
        "        self.feature_layers_num = feature_layers_num\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.model= None\n",
        "\n",
        "    def summary(self, output=None, target=None):\n",
        "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        if target is not None:\n",
        "            os.makedirs(output, exist_ok=True)\n",
        "\n",
        "            \"\"\"with open(os.path.join(output, target), \"w\") as f:\n",
        "                with redirect_stdout(f):\n",
        "                    self.model.summary()\"\"\"\n",
        "\n",
        "    def load_checkpoint(self, target):\n",
        "        \"\"\" Load a model with checkpoint file\"\"\"\n",
        "\n",
        "        if os.path.isfile(target):\n",
        "            if self.model is None:\n",
        "                self.compile()\n",
        "\n",
        "            self.model.load_weights(target)\n",
        "\n",
        "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
        "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
        "\n",
        "        callbacks = [\n",
        "            CSVLogger(\n",
        "                filename=os.path.join(logdir, \"epochs.log\"),\n",
        "                separator=\";\",\n",
        "                append=True),\n",
        "            TensorBoard(\n",
        "                log_dir=logdir,\n",
        "                write_graph=True,\n",
        "                write_images=False,\n",
        "                update_freq=\"epoch\"),\n",
        "            ModelCheckpoint(\n",
        "                filepath=checkpoint,\n",
        "                monitor=monitor,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=verbose),\n",
        "            EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                patience=20,\n",
        "                restore_best_weights=True,\n",
        "                verbose=verbose),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                factor=0.2,\n",
        "                patience=15,\n",
        "                verbose=verbose)\n",
        "        ]\n",
        "\n",
        "\n",
        "        return callbacks\n",
        "    def compile(self, learning_rate=None):\n",
        "        \"\"\"\n",
        "        Configures the NN Model for training/predict.\n",
        "\n",
        "        optimizer: optimizer for training\n",
        "        \"\"\"\n",
        "\n",
        "        # define inputs, outputs and optimizer of the chosen architecture\n",
        "        outs = self.architecture(self.num_channels, self.locked_layers, self.feature_layers_range, self.feature_layers_num, self.learning_rate)\n",
        "        inputs, outputs, optimizer = outs\n",
        "\n",
        "        # create and compile\n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.quad_loss)\n",
        "\n",
        "    def fit(self,\n",
        "            x=None,\n",
        "            y=None,\n",
        "            batch_size=None,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=None,\n",
        "            validation_split=0.0,\n",
        "            validation_data=None,\n",
        "            shuffle=True,\n",
        "            class_weight=None,\n",
        "            sample_weight=None,\n",
        "            initial_epoch=0,\n",
        "            steps_per_epoch=None,\n",
        "            validation_steps=None,\n",
        "            validation_freq=1,\n",
        "            max_queue_size=10,\n",
        "            workers=1,\n",
        "            use_multiprocessing=False,\n",
        "            checkpoint=\"\"):\n",
        "        \"\"\"\n",
        "        Model training on data yielded (fit_generator function has support to generator).\n",
        "        A fit_generator() abstration function of TensorFlow Keras.\n",
        "\n",
        "        Provide x parameter, a generator of the form: yielding x, y.\n",
        "\n",
        "        return: A history object\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.model.fit_generator(generator=x,\n",
        "                           steps_per_epoch=steps_per_epoch,\n",
        "                           epochs=epochs,\n",
        "                           validation_data=validation_data,\n",
        "                           validation_steps=validation_steps,\n",
        "                           verbose=1,\n",
        "                           initial_epoch=initial_epoch,\n",
        "                           callbacks=callbacks)\n",
        "        return out\n",
        "\n",
        "    def predict(self,\n",
        "                x,\n",
        "                batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def quad_loss(y_true, y_pred):\n",
        "        \n",
        "        epsilon = 1e-6\n",
        "        lambda_side_vertex_coord_loss = 1.0\n",
        "        lambda_side_vertex_code_loss = 1.0\n",
        "        lambda_inside_score_loss = 4.0\n",
        "        def smooth_l1_loss(prediction_tensor, target_tensor, weights):\n",
        "            n_q = tf.reshape(quad_norm(target_tensor), tf.shape(weights))\n",
        "            diff = prediction_tensor - target_tensor\n",
        "            abs_diff = tf.abs(diff)\n",
        "            abs_diff_lt_1 = tf.less(abs_diff, 1)\n",
        "            pixel_wise_smooth_l1norm = (tf.reduce_sum(\n",
        "                tf.where(abs_diff_lt_1, 0.5 * tf.square(abs_diff), abs_diff - 0.5),\n",
        "                axis=-1) / n_q) * weights\n",
        "            return pixel_wise_smooth_l1norm\n",
        "\n",
        "\n",
        "        def quad_norm(g_true):\n",
        "            shape = tf.shape(g_true)\n",
        "            delta_xy_matrix = tf.reshape(g_true, [-1, 2, 2])\n",
        "            diff = delta_xy_matrix[:, 0:1, :] - delta_xy_matrix[:, 1:2, :]\n",
        "            square = tf.square(diff)\n",
        "            distance = tf.sqrt(tf.reduce_sum(square, axis=-1))\n",
        "            distance *= 4.0\n",
        "            distance += epsilon\n",
        "            return tf.reshape(distance, shape[:-1])\n",
        "        \n",
        "        \n",
        "        # loss for inside_score\n",
        "        logits = y_pred[:, :, :, :1]\n",
        "        labels = y_true[:, :, :, :1]\n",
        "        # balance positive and negative samples in an image\n",
        "        beta = 1 - tf.reduce_mean(labels)\n",
        "        # first apply sigmoid activation\n",
        "        predicts = tf.nn.sigmoid(logits)\n",
        "        # log +epsilon for stable cal\n",
        "        inside_score_loss = tf.reduce_mean(\n",
        "            -1 * (beta * labels * tf.math.log(predicts + epsilon) +\n",
        "                  (1 - beta) * (1 - labels) * tf.math.log(1 - predicts + epsilon)))\n",
        "        inside_score_loss *= lambda_inside_score_loss\n",
        "\n",
        "        # loss for side_vertex_code\n",
        "        vertex_logits = y_pred[:, :, :, 1:3]\n",
        "        vertex_labels = y_true[:, :, :, 1:3]\n",
        "        vertex_beta = 1 - (tf.reduce_mean(y_true[:, :, :, 1:2])\n",
        "                           / (tf.reduce_mean(labels) + epsilon))\n",
        "        vertex_predicts = tf.nn.sigmoid(vertex_logits)\n",
        "        pos = -1 * vertex_beta * vertex_labels * tf.math.log(vertex_predicts +\n",
        "                                                        epsilon)\n",
        "        neg = -1 * (1 - vertex_beta) * (1 - vertex_labels) * tf.math.log(\n",
        "            1 - vertex_predicts + epsilon)\n",
        "        positive_weights = tf.cast(tf.equal(y_true[:, :, :, 0], 1), tf.float32)\n",
        "        side_vertex_code_loss = \\\n",
        "            tf.reduce_sum(tf.reduce_sum(pos + neg, axis=-1) * positive_weights) / (\n",
        "                    tf.reduce_sum(positive_weights) + epsilon)\n",
        "        side_vertex_code_loss *= lambda_side_vertex_code_loss\n",
        "\n",
        "        # loss for side_vertex_coord delta\n",
        "        g_hat = y_pred[:, :, :, 3:]\n",
        "        g_true = y_true[:, :, :, 3:]\n",
        "        vertex_weights = tf.cast(tf.equal(y_true[:, :, :, 1], 1), tf.float32)\n",
        "        pixel_wise_smooth_l1norm = smooth_l1_loss(g_hat, g_true, vertex_weights)\n",
        "        side_vertex_coord_loss = tf.reduce_sum(pixel_wise_smooth_l1norm) / (\n",
        "                tf.reduce_sum(vertex_weights) + epsilon)\n",
        "        side_vertex_coord_loss *= lambda_side_vertex_coord_loss\n",
        "        \n",
        "        return inside_score_loss + side_vertex_code_loss + side_vertex_coord_loss\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Networks to the text localization\n",
        "\"\"\"\n",
        "\n",
        "def east(num_channels, locked_layers, feature_layers_range, feature_layers_num, learning_rate):\n",
        "    input_img = Input(name='input_img',\n",
        "                               shape=(None, None, num_channels),\n",
        "                               dtype='float32')\n",
        "    vgg16 = VGG16(input_tensor=input_img,\n",
        "                  weights='imagenet',\n",
        "                  include_top=False)\n",
        "    if locked_layers:\n",
        "        # locked first two conv layers\n",
        "        locked_layers = [vgg16.get_layer('block1_conv1'),\n",
        "                         vgg16.get_layer('block1_conv2')]\n",
        "        for layer in locked_layers:\n",
        "            layer.trainable = False\n",
        "    f = [vgg16.get_layer('block%d_pool' % i).output\n",
        "              for i in feature_layers_range]\n",
        "    f.insert(0, None)\n",
        "    diff = feature_layers_range[0] - feature_layers_num\n",
        "    \n",
        "    def g(i):\n",
        "        # i+diff in cfg.feature_layers_range\n",
        "        assert i + diff in feature_layers_range, \\\n",
        "            ('i=%d+diff=%d not in ' % (i, diff)) + \\\n",
        "            str(feature_layers_range)\n",
        "        if i == feature_layers_num:\n",
        "            bn = BatchNormalization()(h(i))\n",
        "            cn = Conv2D(32, 3, activation='relu', padding='same')(bn)\n",
        "            return cn\n",
        "        else:\n",
        "            return UpSampling2D((2, 2))(h(i))\n",
        "\n",
        "    def h(i):\n",
        "        # i+diff in cfg.feature_layers_range\n",
        "        assert i + diff in feature_layers_range, \\\n",
        "            ('i=%d+diff=%d not in ' % (i, diff)) + \\\n",
        "            str(feature_layers_range)\n",
        "        if i == 1:\n",
        "            return f[i]\n",
        "        else:\n",
        "            concat = Concatenate(axis=-1)([g(i - 1), f[i]])\n",
        "            bn1 = BatchNormalization()(concat)\n",
        "            conv_1 = Conv2D(128 // 2 ** (i - 2), 1,\n",
        "                            activation='relu', padding='same',)(bn1)\n",
        "            bn2 = BatchNormalization()(conv_1)\n",
        "            conv_3 = Conv2D(128 // 2 ** (i - 2), 3,\n",
        "                            activation='relu', padding='same',)(bn2)\n",
        "            return conv_3\n",
        "\n",
        "    before_output = g(feature_layers_num)\n",
        "    inside_score = Conv2D(1, 1, padding='same', name='inside_score'\n",
        "                          )(before_output)\n",
        "    side_v_code = Conv2D(2, 1, padding='same', name='side_vertex_code'\n",
        "                         )(before_output)\n",
        "    side_v_coord = Conv2D(4, 1, padding='same', name='side_vertex_coord'\n",
        "                          )(before_output)\n",
        "    east_detect = Concatenate(axis=-1,\n",
        "                              name='east_detect')([inside_score,\n",
        "                                                   side_v_code,\n",
        "                                                   side_v_coord])\n",
        "    \n",
        "    optimizer = Adam(lr=learning_rate, decay=5e-4)\n",
        "    \n",
        "    return (input_img, east_detect, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dYnbAdb6tgw",
        "colab_type": "text"
      },
      "source": [
        "## Predict Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGdCWpOP6sUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw\n",
        "from PIL import ImageQt\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"`y = 1 / (1 + exp(-x))`\"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def cut_text_line(geo, scale_ratio_w, scale_ratio_h, im_array, img_path, s):\n",
        "    geo /= [scale_ratio_w, scale_ratio_h]\n",
        "    p_min = np.amin(geo, axis=0)\n",
        "    p_max = np.amax(geo, axis=0)\n",
        "    min_xy = p_min.astype(int)\n",
        "    max_xy = p_max.astype(int) + 2\n",
        "    sub_im_arr = im_array[min_xy[1]:max_xy[1], min_xy[0]:max_xy[0], :].copy()\n",
        "    for m in range(min_xy[1], max_xy[1]):\n",
        "        for n in range(min_xy[0], max_xy[0]):\n",
        "            if not point_inside_of_quad(n, m, geo, p_min, p_max):\n",
        "                sub_im_arr[m - min_xy[1], n - min_xy[0], :] = 255\n",
        "    sub_im = image.array_to_img(sub_im_arr, scale=False)\n",
        "    \n",
        "    return binarize(sub_im_arr)\n",
        "    \n",
        "    #sub_im.save(img_path + '_subim%d.jpg' % s)\n",
        "\n",
        "\n",
        "def predict_image(detector, img_path, pixel_threshold, quiet=True,gen=None):\n",
        "    #im2,out = gen.sample()\n",
        "    results = []\n",
        "    \n",
        "    img = image.load_img(img_path)\n",
        "    #img = Image.open(img_path)\n",
        "    #img = image.array_to_img(im2)\n",
        "    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n",
        "    img = img.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "    img = image.img_to_array(img)\n",
        "    \n",
        "    img = preprocess_input(img, mode='tf')\n",
        "\n",
        "    b, g, r = img[:,:,0],img[:,:,1],img[:,:,2]\n",
        "    img = np.dstack([r,g,b])\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    print(x.shape)\n",
        "    y = detector.predict(x)\n",
        "    #y = out\n",
        "    y = np.squeeze(y, axis=0)\n",
        "    print(y.shape)#,y[:,:,0],y[:,:,1],y[:,:,2],y[:,:,3],y[:,:,4],y[:,:,5],y[:,:,6])\n",
        "    y[:, :, :3] = sigmoid(y[:, :, :3])\n",
        "    \"\"\"opt = np.get_printoptions()\n",
        "    np.set_printoptions(threshold=np.inf)\n",
        "    #print(np.asarray(y[:,:,1]))\n",
        "    print(np.asarray(y[:,:,2]))\n",
        "    np.set_printoptions(**opt)\"\"\"\n",
        "    cond = np.greater_equal(y[:, :, 0],sigmoid(pixel_threshold))\n",
        "    activation_pixels = np.where(cond)\n",
        "    quad_scores, quad_after_nms = nms(y, activation_pixels)\n",
        "    with Image.open(img_path) as im:\n",
        "        #im = im2\n",
        "        im = image.array_to_img(img)\n",
        "        im_array = image.img_to_array(im.convert('RGB'))\n",
        "        d_wight, d_height = resize_image(im, cfg.max_predict_img_size)\n",
        "        scale_ratio_w = d_wight / im.width\n",
        "        scale_ratio_h = d_height / im.height\n",
        "        im = im.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "        quad_im = im.copy()\n",
        "        draw = ImageDraw.Draw(im)\n",
        "        for i, j in zip(activation_pixels[0], activation_pixels[1]):\n",
        "            px = (j + 0.5) * cfg.pixel_size\n",
        "            py = (i + 0.5) * cfg.pixel_size\n",
        "            line_width, line_color = 1, 'red'\n",
        "            if y[i, j, 1] >= sigmoid(cfg.side_vertex_pixel_threshold):\n",
        "                if y[i, j, 2] < sigmoid(cfg.trunc_threshold):\n",
        "                    line_width, line_color = 2, 'yellow'\n",
        "                elif y[i, j, 2] >= sigmoid(1-cfg.trunc_threshold):\n",
        "                    line_width, line_color = 2, 'green'\n",
        "            draw.line([(px - 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size),\n",
        "                       (px + 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size),\n",
        "                       (px + 0.5 * cfg.pixel_size, py + 0.5 * cfg.pixel_size),\n",
        "                       (px - 0.5 * cfg.pixel_size, py + 0.5 * cfg.pixel_size),\n",
        "                       (px - 0.5 * cfg.pixel_size, py - 0.5 * cfg.pixel_size)],\n",
        "                      width=line_width, fill=line_color)\n",
        "        quad_draw = ImageDraw.Draw(quad_im)\n",
        "        txt_items = []\n",
        "        for score, geo, s in zip(quad_scores, quad_after_nms,\n",
        "                                 range(len(quad_scores))):\n",
        "            if np.amin(score) > 0:\n",
        "                quad_draw.line([tuple(geo[0]),\n",
        "                                tuple(geo[1]),\n",
        "                                tuple(geo[2]),\n",
        "                                tuple(geo[3]),\n",
        "                                tuple(geo[0])], width=2, fill='red')\n",
        "                if cfg.predict_cut_text_line:\n",
        "                    results.append(cut_text_line(geo, scale_ratio_w, scale_ratio_h, im_array,\n",
        "                                  img_path, s))\n",
        "                    \n",
        "                rescaled_geo = geo / [scale_ratio_w, scale_ratio_h]\n",
        "                rescaled_geo_list = np.reshape(rescaled_geo, (8,)).tolist()\n",
        "                txt_item = ','.join(map(str, rescaled_geo_list))\n",
        "                txt_items.append(txt_item + '\\n')\n",
        "            elif not quiet:\n",
        "                print('quad invalid with vertex num less then 4.')\n",
        "        \n",
        "        return results, image.img_to_array(quad_im.convert('RGB'))\n",
        "        \"\"\"if cfg.predict_write2txt and len(txt_items) > 0:\n",
        "            with open(img_path[:-4] + '.txt', 'w') as f_txt:\n",
        "                f_txt.writelines(txt_items)\"\"\"\n",
        "\n",
        "\n",
        "def predict_txt(east_detect, img_path, txt_path, pixel_threshold, quiet=False):\n",
        "    img = image.load_img(img_path)\n",
        "    d_wight, d_height = resize_image(img, cfg.max_predict_img_size)\n",
        "    scale_ratio_w = d_wight / img.width\n",
        "    scale_ratio_h = d_height / img.height\n",
        "    img = img.resize((d_wight, d_height), Image.NEAREST).convert('RGB')\n",
        "    img = image.img_to_array(img)\n",
        "    img = preprocess_input(img, mode='tf')\n",
        "    x = np.expand_dims(img, axis=0)\n",
        "    y = east_detect.predict(x)\n",
        "\n",
        "    y = np.squeeze(y, axis=0)\n",
        "    y[:, :, :3] = sigmoid(y[:, :, :3])\n",
        "    cond = np.greater_equal(y[:, :, 0], pixel_threshold)\n",
        "    activation_pixels = np.where(cond)\n",
        "    quad_scores, quad_after_nms = nms(y, activation_pixels)\n",
        "\n",
        "    txt_items = []\n",
        "    for score, geo in zip(quad_scores, quad_after_nms):\n",
        "        if np.amin(score) > 0:\n",
        "            rescaled_geo = geo / [scale_ratio_w, scale_ratio_h]\n",
        "            rescaled_geo_list = np.reshape(rescaled_geo, (8,)).tolist()\n",
        "            txt_item = ','.join(map(str, rescaled_geo_list))\n",
        "            txt_items.append(txt_item + '\\n')\n",
        "        elif not quiet:\n",
        "            print('quad invalid with vertex num less then 4.')\n",
        "    if cfg.predict_write2txt and len(txt_items) > 0:\n",
        "        with open(txt_path, 'w') as f_txt:\n",
        "            f_txt.writelines(txt_items)\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--path', '-p',\n",
        "                        default='demo/012.png',\n",
        "                        help='image path')\n",
        "    parser.add_argument('--threshold', '-t',\n",
        "                        default=cfg.pixel_threshold,\n",
        "                        help='pixel activation threshold')\n",
        "    return parser.parse_args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4lZxAo4l3N0",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w6Q4Ym5l1e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_reorder_vertexes(xy_list_array):\n",
        "    reorder_xy_list_array = np.zeros_like(xy_list_array)\n",
        "    for xy_list, i in zip(xy_list_array, range(len(xy_list_array))):\n",
        "        reorder_xy_list_array[i] = reorder_vertexes(xy_list)\n",
        "    return reorder_xy_list_array\n",
        "\n",
        "\n",
        "def reorder_vertexes(xy_list):\n",
        "    reorder_xy_list = np.zeros_like(xy_list)\n",
        "    # determine the first point with the smallest x,\n",
        "    # if two has same x, choose that with smallest y,\n",
        "    ordered = np.argsort(xy_list, axis=0)\n",
        "    xmin1_index = ordered[0, 0]\n",
        "    xmin2_index = ordered[1, 0]\n",
        "    if xy_list[xmin1_index, 0] == xy_list[xmin2_index, 0]:\n",
        "        if xy_list[xmin1_index, 1] <= xy_list[xmin2_index, 1]:\n",
        "            reorder_xy_list[0] = xy_list[xmin1_index]\n",
        "            first_v = xmin1_index\n",
        "        else:\n",
        "            reorder_xy_list[0] = xy_list[xmin2_index]\n",
        "            first_v = xmin2_index\n",
        "    else:\n",
        "        reorder_xy_list[0] = xy_list[xmin1_index]\n",
        "        first_v = xmin1_index\n",
        "    # connect the first point to others, the third point on the other side of\n",
        "    # the line with the middle slope\n",
        "    others = list(range(4))\n",
        "    others.remove(first_v)\n",
        "    k = np.zeros((len(others),))\n",
        "    for index, i in zip(others, range(len(others))):\n",
        "        k[i] = (xy_list[index, 1] - xy_list[first_v, 1]) \\\n",
        "                    / (xy_list[index, 0] - xy_list[first_v, 0] + cfg.epsilon)\n",
        "    k_mid = np.argsort(k)[1]\n",
        "    third_v = others[k_mid]\n",
        "    reorder_xy_list[2] = xy_list[third_v]\n",
        "    # determine the second point which on the bigger side of the middle line\n",
        "    others.remove(third_v)\n",
        "    b_mid = xy_list[first_v, 1] - k[k_mid] * xy_list[first_v, 0]\n",
        "    second_v, fourth_v = 0, 0\n",
        "    for index, i in zip(others, range(len(others))):\n",
        "        # delta = y - (k * x + b)\n",
        "        delta_y = xy_list[index, 1] - (k[k_mid] * xy_list[index, 0] + b_mid)\n",
        "        if delta_y > 0:\n",
        "            second_v = index\n",
        "        else:\n",
        "            fourth_v = index\n",
        "    reorder_xy_list[1] = xy_list[second_v]\n",
        "    reorder_xy_list[3] = xy_list[fourth_v]\n",
        "    # compare slope of 13 and 24, determine the final order\n",
        "    k13 = k[k_mid]\n",
        "    k24 = (xy_list[second_v, 1] - xy_list[fourth_v, 1]) / (\n",
        "                xy_list[second_v, 0] - xy_list[fourth_v, 0] + cfg.epsilon)\n",
        "    if k13 < k24:\n",
        "        tmp_x, tmp_y = reorder_xy_list[3, 0], reorder_xy_list[3, 1]\n",
        "        for i in range(2, -1, -1):\n",
        "            reorder_xy_list[i + 1] = reorder_xy_list[i]\n",
        "        reorder_xy_list[0, 0], reorder_xy_list[0, 1] = tmp_x, tmp_y\n",
        "    return reorder_xy_list\n",
        "\n",
        "\n",
        "def resize_image(im, max_img_size=cfg.max_train_img_size):\n",
        "    im_width = np.minimum(im.width, max_img_size)\n",
        "    if im_width == max_img_size < im.width:\n",
        "        im_height = int((im_width / im.width) * im.height)\n",
        "    else:\n",
        "        im_height = im.height\n",
        "    o_height = np.minimum(im_height, max_img_size)\n",
        "    if o_height == max_img_size < im_height:\n",
        "        o_width = int((o_height / im_height) * im_width)\n",
        "    else:\n",
        "        o_width = im_width\n",
        "    d_wight = o_width - (o_width % 32)\n",
        "    d_height = o_height - (o_height % 32)\n",
        "    return d_wight, d_height\n",
        "\n",
        "\n",
        "def point_inside_of_quad(px, py, quad_xy_list, p_min, p_max):\n",
        "    if (p_min[0] <= px <= p_max[0]) and (p_min[1] <= py <= p_max[1]):\n",
        "        xy_list = np.zeros((4, 2))\n",
        "        xy_list[:3, :] = quad_xy_list[1:4, :] - quad_xy_list[:3, :]\n",
        "        xy_list[3] = quad_xy_list[0, :] - quad_xy_list[3, :]\n",
        "        yx_list = np.zeros((4, 2))\n",
        "        yx_list[:, :] = quad_xy_list[:, -1:-3:-1]\n",
        "        a = xy_list * ([py, px] - yx_list)\n",
        "        b = a[:, 0] - a[:, 1]\n",
        "        if np.amin(b) >= 0 or np.amax(b) <= 0:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def point_inside_of_nth_quad(px, py, xy_list, shrink_1, long_edge):\n",
        "    nth = -1\n",
        "    vs = [[[0, 0, 3, 3, 0], [1, 1, 2, 2, 1]],\n",
        "          [[0, 0, 1, 1, 0], [2, 2, 3, 3, 2]]]\n",
        "    for ith in range(2):\n",
        "        quad_xy_list = np.concatenate((\n",
        "            np.reshape(xy_list[vs[long_edge][ith][0]], (1, 2)),\n",
        "            np.reshape(shrink_1[vs[long_edge][ith][1]], (1, 2)),\n",
        "            np.reshape(shrink_1[vs[long_edge][ith][2]], (1, 2)),\n",
        "            np.reshape(xy_list[vs[long_edge][ith][3]], (1, 2))), axis=0)\n",
        "        p_min = np.amin(quad_xy_list, axis=0)\n",
        "        p_max = np.amax(quad_xy_list, axis=0)\n",
        "        if point_inside_of_quad(px, py, quad_xy_list, p_min, p_max):\n",
        "            if nth == -1:\n",
        "                nth = ith\n",
        "            else:\n",
        "                nth = -1\n",
        "                break\n",
        "    return nth\n",
        "\n",
        "\n",
        "def shrink(xy_list, ratio=cfg.shrink_ratio):\n",
        "    if ratio == 0.0:\n",
        "        return xy_list, xy_list\n",
        "    diff_1to3 = xy_list[:3, :] - xy_list[1:4, :]\n",
        "    diff_4 = xy_list[3:4, :] - xy_list[0:1, :]\n",
        "    diff = np.concatenate((diff_1to3, diff_4), axis=0)\n",
        "    dis = np.sqrt(np.sum(np.square(diff), axis=-1))\n",
        "    # determine which are long or short edges\n",
        "    long_edge = int(np.argmax(np.sum(np.reshape(dis, (2, 2)), axis=0)))\n",
        "    short_edge = 1 - long_edge\n",
        "    # cal r length array\n",
        "    r = [np.minimum(dis[i], dis[(i + 1) % 4]) for i in range(4)]\n",
        "    # cal theta array\n",
        "    diff_abs = np.abs(diff)\n",
        "    diff_abs[:, 0] += cfg.epsilon\n",
        "    theta = np.arctan(diff_abs[:, 1] / diff_abs[:, 0])\n",
        "    # shrink two long edges\n",
        "    temp_new_xy_list = np.copy(xy_list)\n",
        "    shrink_edge(xy_list, temp_new_xy_list, long_edge, r, theta, ratio)\n",
        "    shrink_edge(xy_list, temp_new_xy_list, long_edge + 2, r, theta, ratio)\n",
        "    # shrink two short edges\n",
        "    new_xy_list = np.copy(temp_new_xy_list)\n",
        "    shrink_edge(temp_new_xy_list, new_xy_list, short_edge, r, theta, ratio)\n",
        "    shrink_edge(temp_new_xy_list, new_xy_list, short_edge + 2, r, theta, ratio)\n",
        "    return temp_new_xy_list, new_xy_list, long_edge\n",
        "\n",
        "\n",
        "def shrink_edge(xy_list, new_xy_list, edge, r, theta, ratio=cfg.shrink_ratio):\n",
        "    if ratio == 0.0:\n",
        "        return\n",
        "    start_point = edge\n",
        "    end_point = (edge + 1) % 4\n",
        "    long_start_sign_x = np.sign(\n",
        "        xy_list[end_point, 0] - xy_list[start_point, 0])\n",
        "    new_xy_list[start_point, 0] = \\\n",
        "        xy_list[start_point, 0] + \\\n",
        "        long_start_sign_x * ratio * r[start_point] * np.cos(theta[start_point])\n",
        "    long_start_sign_y = np.sign(\n",
        "        xy_list[end_point, 1] - xy_list[start_point, 1])\n",
        "    new_xy_list[start_point, 1] = \\\n",
        "        xy_list[start_point, 1] + \\\n",
        "        long_start_sign_y * ratio * r[start_point] * np.sin(theta[start_point])\n",
        "    # long edge one, end point\n",
        "    long_end_sign_x = -1 * long_start_sign_x\n",
        "    new_xy_list[end_point, 0] = \\\n",
        "        xy_list[end_point, 0] + \\\n",
        "        long_end_sign_x * ratio * r[end_point] * np.cos(theta[start_point])\n",
        "    long_end_sign_y = -1 * long_start_sign_y\n",
        "    new_xy_list[end_point, 1] = \\\n",
        "        xy_list[end_point, 1] + \\\n",
        "        long_end_sign_y * ratio * r[end_point] * np.sin(theta[start_point])\n",
        "\n",
        "\n",
        "arabic = True\n",
        "def adjust_to_see(img):\n",
        "    \"\"\"Rotate and transpose to image visualize (cv2 method or jupyter notebook)\"\"\"\n",
        "\n",
        "    (h, w) = img.shape[:2]\n",
        "    (cX, cY) = (w // 2, h // 2)\n",
        "\n",
        "    M = cv2.getRotationMatrix2D((cX, cY), -90, 1.0)\n",
        "    cos = np.abs(M[0, 0])\n",
        "    sin = np.abs(M[0, 1])\n",
        "\n",
        "    nW = int((h * sin) + (w * cos))\n",
        "    nH = int((h * cos) + (w * sin))\n",
        "\n",
        "    M[0, 2] += (nW / 2) - cX\n",
        "    M[1, 2] += (nH / 2) - cY\n",
        "\n",
        "    img = cv2.warpAffine(img, M, (nW + 1, nH + 1))\n",
        "    img = cv2.warpAffine(img.transpose(), M, (nW, nH))\n",
        "    if arabic:\n",
        "      img = cv2.flip(img,1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def augmentation(imgs,\n",
        "                 rotation_range=0,\n",
        "                 scale_range=0,\n",
        "                 height_shift_range=0,\n",
        "                 width_shift_range=0,\n",
        "                 dilate_range=1,\n",
        "                 erode_range=1):\n",
        "    \"\"\"Apply variations to a list of images (rotate, width and height shift, scale, erode, dilate)\"\"\"\n",
        "\n",
        "    imgs = imgs.astype(np.float32)\n",
        "    _, h, w = imgs.shape\n",
        "\n",
        "    dilate_kernel = np.ones((int(np.random.uniform(1, dilate_range)),), np.uint8)\n",
        "    erode_kernel = np.ones((int(np.random.uniform(1, erode_range)),), np.uint8)\n",
        "    height_shift = np.random.uniform(-height_shift_range, height_shift_range)\n",
        "    rotation = np.random.uniform(-rotation_range, rotation_range)\n",
        "    scale = np.random.uniform(1 - scale_range, 1)\n",
        "    width_shift = np.random.uniform(-width_shift_range, width_shift_range)\n",
        "\n",
        "    trans_map = np.float32([[1, 0, width_shift * w], [0, 1, height_shift * h]])\n",
        "    rot_map = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)\n",
        "\n",
        "    trans_map_aff = np.r_[trans_map, [[0, 0, 1]]]\n",
        "    rot_map_aff = np.r_[rot_map, [[0, 0, 1]]]\n",
        "    affine_mat = rot_map_aff.dot(trans_map_aff)[:2, :]\n",
        "\n",
        "    for i in range(len(imgs)):\n",
        "        imgs[i] = cv2.warpAffine(imgs[i], affine_mat, (w, h), flags=cv2.INTER_NEAREST, borderValue=255)\n",
        "        imgs[i] = cv2.erode(imgs[i], erode_kernel, iterations=1)\n",
        "        imgs[i] = cv2.dilate(imgs[i], dilate_kernel, iterations=1)\n",
        "\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def normalization(imgs):\n",
        "    \"\"\"Normalize list of images\"\"\"\n",
        "\n",
        "    imgs = np.asarray(imgs).astype(np.float32)\n",
        "    _, h, w = imgs.shape\n",
        "\n",
        "    for i in range(len(imgs)):\n",
        "        m, s = cv2.meanStdDev(imgs[i])\n",
        "        imgs[i] = imgs[i] - m[0][0]\n",
        "        imgs[i] = imgs[i] / s[0][0] if s[0][0] > 0 else imgs[i]\n",
        "    \n",
        "    return np.expand_dims(imgs, axis=-1)\n",
        "\n",
        "def preprocess(img, input_size):\n",
        "    \"\"\"Make the process with the `input_size` to the scale resize\"\"\"\n",
        "    \n",
        "    if isinstance(img, str):\n",
        "        img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    if isinstance(img, tuple):\n",
        "        image, boundbox = img\n",
        "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        for i in range(len(boundbox)):\n",
        "            if isinstance(boundbox[i], float):\n",
        "                total = len(img) if i < 2 else len(img[0])\n",
        "                boundbox[i] = int(total * boundbox[i])\n",
        "\n",
        "        img = np.asarray(img[boundbox[0]:boundbox[1], boundbox[2]:boundbox[3]], dtype=np.uint8)\n",
        "    img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY).astype('uint8')\n",
        "    wt, ht, _ = input_size\n",
        "    h, w = np.asarray(img).shape[:2]\n",
        "    f = max((w / wt), (h / ht))\n",
        "\n",
        "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "    img = cv2.resize(img, new_size)\n",
        "    \n",
        "    _, binary = cv2.threshold(img, 254, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Not necessary for the KHATTX dataset, uncomment for IAM or RIMES.\n",
        "    \n",
        "    #if np.sum(img) * 0.8 > np.sum(binary):\n",
        "    #    img = illumination_compensation(img)\n",
        "\n",
        "    #img = remove_cursive_style(img)\n",
        "\n",
        "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
        "    target[0:new_size[1], 0:new_size[0]] = img\n",
        "    img = cv2.transpose(target)\n",
        "    img = cv2.flip(img,0)\n",
        "    return img\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOF6yKTu6wNE",
        "colab_type": "text"
      },
      "source": [
        "## NMS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWEvbS1c4JqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def should_merge(region, i, j):\n",
        "    neighbor = {(i, j - 1)}\n",
        "    return not region.isdisjoint(neighbor)\n",
        "\n",
        "\n",
        "def region_neighbor(region_set):\n",
        "    region_pixels = np.array(list(region_set))\n",
        "    j_min = np.amin(region_pixels, axis=0)[1] - 1\n",
        "    j_max = np.amax(region_pixels, axis=0)[1] + 1\n",
        "    i_m = np.amin(region_pixels, axis=0)[0] + 1\n",
        "    region_pixels[:, 0] += 1\n",
        "    neighbor = {(region_pixels[n, 0], region_pixels[n, 1]) for n in\n",
        "                range(len(region_pixels))}\n",
        "    neighbor.add((i_m, j_min))\n",
        "    neighbor.add((i_m, j_max))\n",
        "    return neighbor\n",
        "\n",
        "\n",
        "def region_group(region_list):\n",
        "    S = [i for i in range(len(region_list))]\n",
        "    D = []\n",
        "    while len(S) > 0:\n",
        "        m = S.pop(0)\n",
        "        if len(S) == 0:\n",
        "            # S has only one element, put it to D\n",
        "            D.append([m])\n",
        "        else:\n",
        "            D.append(rec_region_merge(region_list, m, S))\n",
        "    return D\n",
        "\n",
        "\n",
        "def rec_region_merge(region_list, m, S):\n",
        "    rows = [m]\n",
        "    tmp = []\n",
        "    for n in S:\n",
        "        if not region_neighbor(region_list[m]).isdisjoint(region_list[n]) or \\\n",
        "                not region_neighbor(region_list[n]).isdisjoint(region_list[m]):\n",
        "            tmp.append(n)\n",
        "    for d in tmp:\n",
        "        S.remove(d)\n",
        "    for e in tmp:\n",
        "        rows.extend(rec_region_merge(region_list, e, S))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def nms(predict, activation_pixels, threshold=cfg.side_vertex_pixel_threshold):\n",
        "    region_list = []\n",
        "    for i, j in zip(activation_pixels[0], activation_pixels[1]):\n",
        "        merge = False\n",
        "        for k in range(len(region_list)):\n",
        "            if should_merge(region_list[k], i, j):\n",
        "                region_list[k].add((i, j))\n",
        "                merge = True\n",
        "        if not merge:\n",
        "            region_list.append({(i, j)})\n",
        "    D = region_group(region_list)\n",
        "    quad_list = np.zeros((len(D), 4, 2))\n",
        "    score_list = np.zeros((len(D), 4))\n",
        "    for group, g_th in zip(D, range(len(D))):\n",
        "        total_score = np.zeros((4, 2))\n",
        "        for row in group:\n",
        "            for ij in region_list[row]:\n",
        "                score = predict[ij[0], ij[1], 1]\n",
        "                if score >= sigmoid(threshold):\n",
        "                    ith_score = predict[ij[0], ij[1], 2:3]\n",
        "                    if not (sigmoid(cfg.trunc_threshold) <= ith_score < \n",
        "                            sigmoid(1-cfg.trunc_threshold)):\n",
        "                        ith = int(np.around(ith_score))\n",
        "                        total_score[ith * 2:(ith + 1) * 2] += score\n",
        "                        px = (ij[1] + 0.5) * cfg.pixel_size\n",
        "                        py = (ij[0] + 0.5) * cfg.pixel_size\n",
        "                        p_v = [px, py] + np.reshape(predict[ij[0], ij[1], 3:7],\n",
        "                                              (2, 2))\n",
        "                        quad_list[g_th, ith * 2:(ith + 1) * 2] += score * p_v\n",
        "                    \n",
        "                    #print(sigmoid(cfg.trunc_threshold), ith_score, str(sigmoid(1-cfg.trunc_threshold)))\n",
        "        score_list[g_th] = total_score[:, 0]\n",
        "        quad_list[g_th] /= (total_score + cfg.epsilon)\n",
        "    return score_list, quad_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVl2GVEW6z4O",
        "colab_type": "text"
      },
      "source": [
        "## Binarizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66ItPe8b62gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def binarize (img_path):\n",
        "    Use_MedianFilter = True\n",
        "    # Getting the un-binarized image.\n",
        "    #raw_image=cv2.imread(img_path,0)\n",
        "    raw_image = img_path\n",
        "    if(len(raw_image.shape)!=2):\n",
        "        grayScale=cv2.cvtColor(raw_image,cv2.COLOR_BGR2GRAY).astype('uint8')\n",
        "    else:\n",
        "        grayScale=raw_image.astype('uint8')\n",
        "\n",
        "    #Getting the height and width of the Image.\n",
        "    m,n = grayScale.shape\n",
        "    #cv2_imshow(grayScale)\n",
        "    #print(grayScale.shape)\n",
        "    high_thresh, thresh_im = cv2.threshold(grayScale, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    lowThresh = 0.5*high_thresh\n",
        "    \n",
        "    #Applying Canny Edge Detector to the Image.\n",
        "    canny_Image = cv2.Canny(grayScale,350,700)\n",
        "   \n",
        "    kernel = np.ones((3,3),np.uint8)\n",
        "    # cv2.imshow('Canny',canny_Image)\n",
        "    # cv2.waitKey(0)\n",
        "    \n",
        "    #Filling the Blobs Using the imFill Function.\n",
        "    closing = imFill(canny_Image)\n",
        "    #cv2.imshow('imfill image',closing) \n",
        "    #cv2.waitKey(0)\n",
        "\n",
        "    #Finding Index Values in Image Where Pixels are White.  \n",
        "    inds = np.where(closing==255)\n",
        "\n",
        "    #Finding Index Values in Image Where Pixels are Black.  \n",
        "    inds2 = np.where(closing==0)\n",
        "\n",
        "    #Comparing  with original Image to obtain the pixel values for text.  \n",
        "    pix_val = grayScale[inds]\n",
        "\n",
        "    #Comparing  with original Image to obtain the pixel values for back.  \n",
        "    pix_val_back = grayScale[inds2]\n",
        "    \n",
        "    #Taking Median of Text and back and then comparing them.\n",
        "    median_text=np.median(pix_val,axis=0)\n",
        "    median_back=np.median(pix_val_back,axis=0)\n",
        "    if median_text>median_back:\n",
        "        pre_binarize = cv2.bitwise_not(grayScale)\n",
        "    else:\n",
        "        pre_binarize=grayScale\n",
        " \n",
        "    #Sharpen the Image.\n",
        "    sharpen_kernel = np.array([[0,-0.9,0], [-1,4.9,-1], [0,-1,-0]])\n",
        "    new_img = cv2.filter2D(pre_binarize, -1, sharpen_kernel)\n",
        "\n",
        "    #Padding the image to remove unecessary lines. \n",
        "    #new_img = np.pad(pre_binarize,(9,9),\"constant\",constant_values=(255,255))\n",
        "\n",
        "    img= apply_threshold(new_img,thresholder(new_img))\n",
        "\n",
        "    if Use_MedianFilter:\n",
        "        img = cv2.medianBlur(img,3)\n",
        "  \n",
        "    #name = os.path.basename(img_path)\n",
        "    #img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    wt, ht, _ = (1024,64,0)\n",
        "    h, w = np.asarray(img).shape[:2]\n",
        "    f = max((w / wt), (h / ht))\n",
        "\n",
        "    new_size = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
        "    img = cv2.resize(img, new_size)\n",
        "\n",
        "    target = np.ones([ht, wt], dtype=np.uint8) * 255\n",
        "    target[0:new_size[1], 0:new_size[0]] = img\n",
        "    img = cv2.transpose(target)\n",
        "    img = cv2.flip(img,0)\n",
        "    \n",
        "    #np.expand_dims(img,-1)\n",
        "    #img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
        "    return img\n",
        "\n",
        "def thresholder(img, w_size=15,k=0.5):\n",
        "    # Obtaining rows and cols\n",
        "    rows, cols = img.shape\n",
        "    i_rows, i_cols = rows + 1, cols + 1\n",
        "\n",
        "    # Computing integral images\n",
        "    integ = np.zeros((i_rows, i_cols), np.float)\n",
        "    sqr_integral = np.zeros((i_rows, i_cols), np.float)\n",
        "\n",
        "    integ[1:, 1:] = np.cumsum(np.cumsum(img.astype(np.float), axis=0), axis=1)\n",
        "    sqr_img = np.square(img.astype(np.float))\n",
        "    sqr_integral[1:, 1:] = np.cumsum(np.cumsum(sqr_img, axis=0), axis=1)\n",
        "\n",
        "    # Defining grid\n",
        "    x, y = np.meshgrid(np.arange(1, i_cols), np.arange(1, i_rows))\n",
        "\n",
        "    # Obtaining local coordinates\n",
        "    hw_size = w_size // 2\n",
        "    x1 = (x - hw_size).clip(1, cols)\n",
        "    x2 = (x + hw_size).clip(1, cols)\n",
        "    y1 = (y - hw_size).clip(1, rows)\n",
        "    y2 = (y + hw_size).clip(1, rows)\n",
        "\n",
        "    # Obtaining local areas size\n",
        "    l_size = (y2 - y1 + 1) * (x2 - x1 + 1)\n",
        "\n",
        "    # Computing sums\n",
        "    sums = (integ[y2, x2] - integ[y2, x1 - 1] -\n",
        "            integ[y1 - 1, x2] + integ[y1 - 1, x1 - 1])\n",
        "    sqr_sums = (sqr_integral[y2, x2] - sqr_integral[y2, x1 - 1] -\n",
        "                sqr_integral[y1 - 1, x2] + sqr_integral[y1 - 1, x1 - 1])\n",
        "\n",
        "    # Computing local means\n",
        "    means = sums / l_size\n",
        "\n",
        "    # Computing local standard deviation\n",
        "    stds = np.sqrt(sqr_sums / l_size - np.square(means))\n",
        "\n",
        "    # Computing min and max values\n",
        "    max_std = np.max(stds)\n",
        "    min_v = np.min(img)\n",
        "\n",
        "    # Computing thresholds\n",
        "    thresholds = ((1.0 - k) * means + k * min_v + k * stds /\n",
        "                  max_std * (means - min_v))\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def imFill(im_in):\n",
        "    \n",
        "    # Threshold.\n",
        "    # Set values equal to or above 220 to 0.\n",
        "    # Set values below 220 to 255.\n",
        "    \n",
        "    # th, im_th = cv2.threshold(im_in, 220, 255, cv2.THRESH_BINARY_INV);\n",
        "    th, im_th = cv2.threshold(im_in, 0, 128, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Copy the thresholded image.\n",
        "    im_floodfill = im_th.copy()\n",
        "    \n",
        "    # Mask used to flood filling.\n",
        "    # Notice the size needs to be 2 pixels than the image.\n",
        "    h, w = im_th.shape[:2]\n",
        "    mask = np.zeros((h+2, w+2), np.uint8)\n",
        "    \n",
        "    # Floodfill from point (0, 0)\n",
        "    cv2.floodFill(im_floodfill, mask, (0,0), 255)\n",
        "    \n",
        "    # Invert floodfilled image\n",
        "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
        "\n",
        "    # Combine the two images to get the foreground.\n",
        "    im_out = im_th | im_floodfill_inv\n",
        "\n",
        "    return im_out   \n",
        "def apply_threshold(img, threshold=128, wp_val=255):\n",
        "    #wp_val is the white pixel value.\n",
        "    return ((img >= threshold) * wp_val).astype(np.uint8)\n",
        "   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UO4YjCx04K5a",
        "colab_type": "text"
      },
      "source": [
        "# Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0_MjwpcKwma",
        "colab_type": "text"
      },
      "source": [
        "## Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJg4wxalKyVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Gated implementations\n",
        "    GatedConv2D: Introduce a Conv2D layer (same number of filters) to multiply with its sigmoid activation.\n",
        "    FullGatedConv2D: Introduce a Conv2D to extract features (linear and sigmoid), making a full gated process.\n",
        "                     This process will double number of filters to make one convolutional process.\n",
        "\"\"\"\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Multiply, Activation\n",
        "\n",
        "\n",
        "class GatedConv2D(Conv2D):\n",
        "    \"\"\"Gated Convolutional Class\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GatedConv2D, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply gated convolution\"\"\"\n",
        "\n",
        "        output = super(GatedConv2D, self).call(inputs)\n",
        "        linear = Activation(\"linear\")(inputs)\n",
        "        sigmoid = Activation(\"sigmoid\")(output)\n",
        "\n",
        "        return Multiply()([linear, sigmoid])\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return the config of the layer\"\"\"\n",
        "\n",
        "        config = super(GatedConv2D, self).get_config()\n",
        "        return config\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Tensorflow Keras layer implementation of the gated convolution.\n",
        "    Args:\n",
        "        filters (int): Number of output filters.\n",
        "        kwargs: Other Conv2D keyword arguments.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class FullGatedConv2D(Conv2D):\n",
        "    \"\"\"Gated Convolutional Class\"\"\"\n",
        "\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(FullGatedConv2D, self).__init__(filters=filters * 2, **kwargs)\n",
        "        self.nb_filters = filters\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Apply gated convolution\"\"\"\n",
        "\n",
        "        output = super(FullGatedConv2D, self).call(inputs)\n",
        "        linear = Activation(\"linear\")(output[:, :, :, :self.nb_filters])\n",
        "        sigmoid = Activation(\"sigmoid\")(output[:, :, :, self.nb_filters:])\n",
        "\n",
        "        return Multiply()([linear, sigmoid])\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"Compute shape of layer output\"\"\"\n",
        "\n",
        "        output_shape = super(FullGatedConv2D, self).compute_output_shape(input_shape)\n",
        "        return tuple(output_shape[:3]) + (self.nb_filters,)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Return the config of the layer\"\"\"\n",
        "\n",
        "        config = super(FullGatedConv2D, self).get_config()\n",
        "        config['nb_filters'] = self.nb_filters\n",
        "        del config['filters']\n",
        "        return config"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDqOd4i-i7pJ",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yPTuIFwi7IP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Handwritten Text Recognition Neural Network\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from contextlib import redirect_stdout\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, Bidirectional, LSTM, GRU, Dense\n",
        "from tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU, PReLU\n",
        "from tensorflow.keras.layers import Input, Add, Activation, Lambda, MaxPooling2D, Reshape\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "NNModel Class\n",
        "The NNModel class use Tensorflow 2 Keras module for the use of the\n",
        "Connectionist Temporal Classification (CTC) with the Hadwritten Text Recognition.\n",
        "\n",
        "x is the input features and y the labels.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class NNModel_rec:\n",
        "\n",
        "    def __init__(self,\n",
        "                 architecture,\n",
        "                 input_size,\n",
        "                 vocab_size,\n",
        "                 greedy=False,\n",
        "                 beam_width=10,\n",
        "                 top_paths=1):\n",
        "        \"\"\"\n",
        "        Initialization of a NN Model.\n",
        "\n",
        "        parameters:\n",
        "            architecture: option of the architecture model to build and compile\n",
        "            greedy, beam_width, top_paths: Parameters of the CTC decoding\n",
        "        \"\"\"\n",
        "\n",
        "        self.architecture = globals()[architecture]\n",
        "        self.input_size = input_size\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.model = None\n",
        "        self.greedy = greedy\n",
        "        self.beam_width = beam_width\n",
        "        self.top_paths = max(1, top_paths)\n",
        "    def summary(self, output=None, target=None):\n",
        "        \"\"\"Show/Save model structure (summary)\"\"\"\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        if target is not None:\n",
        "            os.makedirs(output, exist_ok=True)\n",
        "\n",
        "            with open(os.path.join(output, target), \"w\") as f:\n",
        "                with redirect_stdout(f):\n",
        "                    self.model.summary()\n",
        "\n",
        "    def load_checkpoint(self, target):\n",
        "        \"\"\" Load a model with checkpoint file\"\"\"\n",
        "\n",
        "        if os.path.isfile(target):\n",
        "            if self.model is None:\n",
        "                self.compile()\n",
        "\n",
        "            self.model.load_weights(target)\n",
        "\n",
        "    def get_callbacks(self, logdir, checkpoint, monitor=\"val_loss\", verbose=0):\n",
        "        \"\"\"Setup the list of callbacks for the model\"\"\"\n",
        "\n",
        "        callbacks = [\n",
        "            CSVLogger(\n",
        "                filename=os.path.join(logdir, \"epochs.log\"),\n",
        "                separator=\";\",\n",
        "                append=True),\n",
        "            TensorBoard(\n",
        "                log_dir=logdir,\n",
        "                histogram_freq=10,\n",
        "                profile_batch=0,\n",
        "                write_graph=True,\n",
        "                write_images=False,\n",
        "                update_freq=\"epoch\"),\n",
        "            ModelCheckpoint(\n",
        "                filepath=checkpoint,\n",
        "                monitor=monitor,\n",
        "                save_best_only=True,\n",
        "                save_weights_only=True,\n",
        "                verbose=verbose),\n",
        "            EarlyStopping(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                patience=20, # Change this if the training keeps stopping, happened to me earlier.\n",
        "                restore_best_weights=True,\n",
        "                verbose=verbose),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor=monitor,\n",
        "                min_delta=1e-8,\n",
        "                factor=0.2,\n",
        "                patience=15,\n",
        "                verbose=verbose)\n",
        "        ]\n",
        "\n",
        "        return callbacks\n",
        "\n",
        "    def compile(self, learning_rate=None):\n",
        "        \"\"\"\n",
        "        Configures the NN Model for training/predict.\n",
        "\n",
        "        optimizer: optimizer for training\n",
        "        \"\"\"\n",
        "\n",
        "        # define inputs, outputs and optimizer of the chosen architecture\n",
        "        outs = self.architecture(self.input_size, self.vocab_size + 1, learning_rate)\n",
        "        inputs, outputs, optimizer = outs\n",
        "\n",
        "        # create and compile\n",
        "        self.model = Model(inputs=inputs, outputs=outputs)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.ctc_loss_lambda_func)\n",
        "\n",
        "    def fit(self,\n",
        "            x=None,\n",
        "            y=None,\n",
        "            batch_size=None,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=None,\n",
        "            validation_split=0.0,\n",
        "            validation_data=None,\n",
        "            shuffle=True,\n",
        "            class_weight=None,\n",
        "            sample_weight=None,\n",
        "            initial_epoch=0,\n",
        "            steps_per_epoch=None,\n",
        "            validation_steps=None,\n",
        "            validation_freq=1,\n",
        "            max_queue_size=10,\n",
        "            workers=1,\n",
        "            use_multiprocessing=False,\n",
        "            **kwargs):\n",
        "        \"\"\"\n",
        "        Model training on data yielded (fit function has support to generator).\n",
        "        A fit() abstration function of TensorFlow 2.\n",
        "\n",
        "        Provide x parameter of the form: yielding (x, y, sample_weight).\n",
        "\n",
        "        return: A history object\n",
        "        \"\"\"\n",
        "\n",
        "        out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n",
        "                             callbacks=callbacks, validation_split=validation_split,\n",
        "                             validation_data=validation_data, shuffle=shuffle,\n",
        "                             class_weight=class_weight, sample_weight=sample_weight,\n",
        "                             initial_epoch=initial_epoch, steps_per_epoch=steps_per_epoch,\n",
        "                             validation_steps=validation_steps, validation_freq=validation_freq,\n",
        "                             max_queue_size=max_queue_size, workers=workers,\n",
        "                             use_multiprocessing=use_multiprocessing, **kwargs)\n",
        "        return out\n",
        "\n",
        "    def predict(self,\n",
        "                x,\n",
        "                batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False,\n",
        "                ctc_decode=True):\n",
        "        \"\"\"\n",
        "        Model predicting on data yielded (predict function has support to generator).\n",
        "        A predict() abstration function of TensorFlow 2.\n",
        "\n",
        "        Provide x parameter of the form: yielding [x].\n",
        "\n",
        "        returns: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
        "        \"\"\"\n",
        "\n",
        "        self.model._make_predict_function()\n",
        "\n",
        "        if verbose == 1:\n",
        "            print(\"Model Predict\")\n",
        "\n",
        "        out = self.model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
        "                                 callbacks=callbacks, max_queue_size=max_queue_size,\n",
        "                                 workers=workers, use_multiprocessing=use_multiprocessing)\n",
        "\n",
        "        if not ctc_decode:\n",
        "            return np.log(out.clip(min=1e-8))\n",
        "\n",
        "        steps_done = 0\n",
        "        if verbose == 1:\n",
        "            print(\"CTC Decode\")\n",
        "            progbar = tf.keras.utils.Progbar(target=steps)\n",
        "\n",
        "        batch_size = int(np.ceil(len(out) / steps))\n",
        "        input_length = len(max(out, key=len))\n",
        "\n",
        "        predicts, probabilities = [], []\n",
        "\n",
        "        while steps_done < steps:\n",
        "            index = steps_done * batch_size\n",
        "            until = index + batch_size\n",
        "\n",
        "            x_test = np.asarray(out[index:until])\n",
        "            x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
        "\n",
        "            decode, log = K.ctc_decode(x_test,\n",
        "                                       x_test_len,\n",
        "                                       greedy=self.greedy,\n",
        "                                       beam_width=self.beam_width,\n",
        "                                       top_paths=self.top_paths)\n",
        "\n",
        "            probabilities.extend([np.exp(x) for x in log])\n",
        "            decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
        "            predicts.extend(np.swapaxes(decode, 0, 1))\n",
        "\n",
        "            steps_done += 1\n",
        "            if verbose == 1:\n",
        "                progbar.update(steps_done)\n",
        "\n",
        "        return (predicts, probabilities)\n",
        "\n",
        "    @staticmethod\n",
        "    def ctc_loss_lambda_func(y_true, y_pred):\n",
        "        \"\"\"Function for computing the CTC loss\"\"\"\n",
        "\n",
        "        if len(y_true.shape) > 2:\n",
        "            y_true = tf.squeeze(y_true)\n",
        "\n",
        "        # y_pred.shape = (batch_size, string_length, alphabet_size_1_hot_encoded)\n",
        "        # output of every model is softmax\n",
        "        # so sum across alphabet_size_1_hot_encoded give 1\n",
        "        #               string_length give string length\n",
        "        input_length = tf.math.reduce_sum(y_pred, axis=-1, keepdims=False)\n",
        "        input_length = tf.math.reduce_sum(input_length, axis=-1, keepdims=True)\n",
        "\n",
        "        # y_true strings are padded with 0\n",
        "        # so sum of non-zero gives number of characters in this string\n",
        "        label_length = tf.math.count_nonzero(y_true, axis=-1, keepdims=True, dtype=\"int64\")\n",
        "\n",
        "        loss = K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "        # average loss across all entries in the batch\n",
        "        loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Networks to the Handwritten Text Recognition Model\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def bluche(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Gated Convolucional Recurrent Neural Network by Bluche et al.\n",
        "\n",
        "    Reference:\n",
        "        Bluche, T., Messina, R.:\n",
        "        Gated convolutional recurrent neural networks for multilingual handwriting recognition.\n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "    cnn = Reshape((input_size[0] // 2, input_size[1] // 2, input_size[2] * 4))(input_data)\n",
        "\n",
        "    cnn = Conv2D(filters=8, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(2,4), strides=(2,4), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=64, kernel_size=(2,4), strides=(2,4), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = GatedConv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), padding=\"same\", activation=\"tanh\")(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(1,4), strides=(1,4), padding=\"valid\")(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
        "    blstm = Dense(units=128, activation=\"tanh\")(blstm)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=128, return_sequences=True))(blstm)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 4e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)\n",
        "\n",
        "\n",
        "def puigcerver(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Convolutional Recurrent Neural Network by Puigcerver et al.\n",
        "\n",
        "    Reference:\n",
        "        Joan Puigcerver.\n",
        "        Are multidimensional recurrent layers really necessary for handwritten text recognition?\n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding=\"same\")(input_data)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "    cnn = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "    cnn = Conv2D(filters=80, kernel_size=(3,3), strides=(1,1), padding=\"same\")(cnn)\n",
        "    cnn = BatchNormalization()(cnn)\n",
        "    cnn = LeakyReLU(alpha=0.01)(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    blstm = Reshape((shape[1], shape[2] * shape[3]))(cnn)\n",
        "\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "    blstm = Bidirectional(LSTM(units=256, return_sequences=True, dropout=0.5))(blstm)\n",
        "\n",
        "    blstm = Dropout(rate=0.5)(blstm)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(blstm)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 3e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)\n",
        "\n",
        "def yazeed(input_size, d_model, learning_rate):\n",
        "    \"\"\"\n",
        "    Gated Convolutional Recurrent Neural Network by Yazeed Mohi-Eldeen Sabil.\n",
        "    Based on the Flor et al architecture, modified some portions to fit the KHATT dataset. \n",
        "    \"\"\"\n",
        "\n",
        "    input_data = Input(name=\"input\", shape=input_size)\n",
        "\n",
        "    cnn = Conv2D(filters=16, kernel_size=(3,3), strides=(2,2), padding=\"same\", kernel_initializer=\"he_uniform\")(input_data)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=16, kernel_size=(3,3), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=32, kernel_size=(3,3), padding=\"same\")(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=40, kernel_size=(2,4), strides=(2,4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=40, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=48, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=56, kernel_size=(2,4), strides=(2,4), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "    cnn = FullGatedConv2D(filters=56, kernel_size=(3,3), padding=\"same\", kernel_constraint=MaxNorm(4, [0,1,2]))(cnn)\n",
        "    cnn = Dropout(rate=0.2)(cnn)\n",
        "\n",
        "    cnn = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding=\"same\", kernel_initializer=\"he_uniform\")(cnn)\n",
        "    cnn = PReLU(shared_axes=[1,2])(cnn)\n",
        "    cnn = BatchNormalization(renorm=True)(cnn)\n",
        "\n",
        "    cnn = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding=\"valid\")(cnn)\n",
        "\n",
        "    shape = cnn.get_shape()\n",
        "    nb_units = shape[2] * shape[3]\n",
        "\n",
        "    bgru = Reshape((shape[1], nb_units))(cnn)\n",
        "\n",
        "    bgru = Bidirectional(GRU(units=nb_units, return_sequences=True, dropout=0.5))(bgru)\n",
        "    bgru = Dense(units=nb_units * 2)(bgru)\n",
        "\n",
        "    bgru = Bidirectional(GRU(units=nb_units, return_sequences=True, dropout=0.5))(bgru)\n",
        "    output_data = Dense(units=d_model, activation=\"softmax\")(bgru)\n",
        "\n",
        "    if learning_rate is None:\n",
        "        learning_rate = 5e-4\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    return (input_data, output_data, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAa9RL9b-GTa",
        "colab_type": "text"
      },
      "source": [
        "## Predict Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqrt5fyw4MpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_text(model,\n",
        "              x,\n",
        "              batch_size=None,\n",
        "                verbose=0,\n",
        "                steps=1,\n",
        "                callbacks=None,\n",
        "                max_queue_size=10,\n",
        "                workers=1,\n",
        "                use_multiprocessing=False,\n",
        "                ctc_decode=True):\n",
        "      \"\"\"\n",
        "      Model predicting on data yielded (predict function has support to generator).\n",
        "      A predict() abstration function of TensorFlow 2.\n",
        "\n",
        "      Provide x parameter of the form: yielding [x].\n",
        "\n",
        "      returns: raw data on `ctc_decode=False` or CTC decode on `ctc_decode=True` (both with probabilities)\n",
        "      \"\"\"\n",
        "      x = normalization(x)\n",
        "      model._make_predict_function()\n",
        "\n",
        "      \n",
        "      print(\"Model Predict\")\n",
        "      out = model.predict(x=x, batch_size=batch_size, verbose=verbose, steps=steps,\n",
        "                                callbacks=callbacks, max_queue_size=max_queue_size,\n",
        "                                workers=workers, use_multiprocessing=use_multiprocessing)\n",
        "\n",
        "      if not ctc_decode:\n",
        "          return np.log(out.clip(min=1e-8))\n",
        "\n",
        "      steps_done = 0\n",
        "      if verbose == 1:\n",
        "          print(\"CTC Decode\")\n",
        "          progbar = tf.keras.utils.Progbar(target=steps)\n",
        "\n",
        "      batch_size = int(np.ceil(len(out) / steps))\n",
        "      input_length = len(max(out, key=len))\n",
        "\n",
        "      predicts, probabilities = [], []\n",
        "      while steps_done < steps:\n",
        "          index = steps_done * batch_size\n",
        "          until = index + batch_size\n",
        "\n",
        "          x_test = np.asarray(out[index:until])\n",
        "          x_test_len = np.asarray([input_length for _ in range(len(x_test))])\n",
        "\n",
        "          decode, log = K.ctc_decode(x_test,\n",
        "                                      x_test_len,\n",
        "                                      greedy=cfg.greedy,\n",
        "                                      beam_width=cfg.beam_width,\n",
        "                                      top_paths=cfg.top_paths)\n",
        "\n",
        "          probabilities.extend([np.exp(x) for x in log])\n",
        "          decode = [[[int(p) for p in x if p != -1] for x in y] for y in decode]\n",
        "          predicts.extend(np.swapaxes(decode, 0, 1))\n",
        "\n",
        "          steps_done += 1\n",
        "          if verbose == 1:\n",
        "              progbar.update(steps_done)\n",
        "      return (predicts, probabilities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNYC1Uq1-CLf",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGy4XcPy9_Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import cv2\n",
        "import html\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "class Tokenizer():\n",
        "    \"\"\"Manages tokens functions and charset/dictionary properties\"\"\"\n",
        "\n",
        "    def __init__(self, chars, max_text_length=128):\n",
        "        self.PAD_TK, self.UNK_TK = \"¶\", \"¤\" # PAD = Blank white space before and after text, UNK = Unknown character\n",
        "        self.chars = (self.PAD_TK + self.UNK_TK + chars)\n",
        "\n",
        "        self.PAD = self.chars.find(self.PAD_TK)\n",
        "        self.UNK = self.chars.find(self.UNK_TK)\n",
        "\n",
        "        self.vocab_size = len(self.chars)\n",
        "        self.maxlen = max_text_length\n",
        "    def encode(self, text):\n",
        "        \"\"\"Encode text to vector\"\"\"\n",
        "\n",
        "        #text = unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
        "        text = \" \".join(text.split())\n",
        "        encoded = []\n",
        "        for item in text:\n",
        "            index = self.chars.find(item)\n",
        "            index = self.UNK if index == -1 else index\n",
        "            encoded.append(index)\n",
        "        \n",
        "        return np.asarray(encoded)\n",
        "\n",
        "    def decode(self, text):\n",
        "        \"\"\"Decode vector to text\"\"\"\n",
        "\n",
        "        decoded = \"\".join([self.chars[int(x)] for x in text if x > -1])\n",
        "        decoded = self.remove_tokens(decoded)\n",
        "        decoded = text_standardize(decoded)\n",
        "\n",
        "        return decoded\n",
        "\n",
        "    def remove_tokens(self, text):\n",
        "        \"\"\"Remove tokens (PAD) from text\"\"\"\n",
        "\n",
        "        return text.replace(self.PAD_TK, \"\")\n",
        "\n",
        "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
        "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
        "    chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
        "    chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
        "RE_RESERVED_CHAR_FILTER = re.compile(r'[¶¤«»]', re.UNICODE)\n",
        "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
        "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
        "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}]'.format(re.escape(string.punctuation)), re.UNICODE)\n",
        "\n",
        "LEFT_PUNCTUATION_FILTER = \"\"\"!%&),.:;<=>?@\\\\]^_`|}~\"\"\"\n",
        "RIGHT_PUNCTUATION_FILTER = \"\"\"\"(/<=>@[\\\\^_`{|~\"\"\"\n",
        "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)\n",
        "\n",
        "\n",
        "def text_standardize(text):\n",
        "    \"\"\"Organize/add spaces around punctuation marks\"\"\"\n",
        "\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    text = html.unescape(text).replace(\"\\\\n\", \"\").replace(\"\\\\t\", \"\")\n",
        "\n",
        "    text = RE_RESERVED_CHAR_FILTER.sub(\"\", text)\n",
        "    text = RE_DASH_FILTER.sub(\"-\", text)\n",
        "    text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
        "    text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
        "    text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
        "    text = RE_BASIC_CLEANER.sub(\"\", text)\n",
        "\n",
        "    text = text.lstrip(LEFT_PUNCTUATION_FILTER)\n",
        "    text = text.rstrip(RIGHT_PUNCTUATION_FILTER)\n",
        "    text = text.translate(str.maketrans({c: \" \"+str(c)+\" \" for c in string.punctuation}))\n",
        "    text = NORMALIZE_WHITESPACE_REGEX.sub(\" \", text.strip())\n",
        "\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp9nBpud4ORp",
        "colab_type": "text"
      },
      "source": [
        "# Replacement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6HAt0bM4Qtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}